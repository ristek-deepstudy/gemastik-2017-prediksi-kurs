{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier as GB\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "from Database import Database\n",
    "from numpy import array\n",
    "from sklearn.naive_bayes import MultinomialNB as NB\n",
    "import sqlite3\n",
    "import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('berita.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"SELECT Text,Date,Clock,Sentiment From Berita \")\n",
    "result = c.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Database()\n",
    "data = {}\n",
    "label = {}\n",
    "for I in result:\n",
    "    session = d.cariSesi(I[1],I[2])[0]\n",
    "    if session not in data:\n",
    "        data[session] = []\n",
    "        label[session] = I[3]\n",
    "    sentence = I[0]\n",
    "    newSentence = \"\"\n",
    "    for J in sentence:\n",
    "        if J.isalpha():\n",
    "            newSentence += J\n",
    "        else:\n",
    "            newSentence += \" \"\n",
    "    data[session].append(newSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sorting the chronological order\n",
    "\n",
    "#MeTooLazySoMeBubbleSort\n",
    "\n",
    "chronology = list(data.keys())\n",
    "\n",
    "for I in range(len(data.keys())):\n",
    "    for J in range(I+1,len(data.keys())):\n",
    "        if chronology[I] > chronology[J]:\n",
    "            chronology[I],chronology[J] = chronology[J],chronology[I]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = [chronology[len(chronology)*I//6-1] for I in range(1,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balancedTrain(X,y,mode):\n",
    "    '''\n",
    "    mode has two options:\n",
    "    -> 'CV'\n",
    "    then balancedTrain would suit it undersampling method for \n",
    "    cross validation\n",
    "    \n",
    "    -> 'Boosting'\n",
    "    then balancedTrain would suit is sampling method for\n",
    "    boosting    \n",
    "    '''\n",
    "    assert mode == 'CV' or mode == 'Boosting'\n",
    "    balancedX = []\n",
    "    balancedY = []\n",
    "    \n",
    "    index = {}\n",
    "    \n",
    "    assert len(X) == len(y)\n",
    "    \n",
    "    #Mencatat indeks mana yang positif dan mana yang negatif\n",
    "    for I in range(len(X)):\n",
    "        if y[I] not in index:\n",
    "            index[y[I]] = []\n",
    "        index[y[I]].append(I)\n",
    "    \n",
    "    minimumPoint = min([len(I) for I in index.values()])    \n",
    "    \n",
    "    #Memastikan jumlah (+) dan (-) sama\n",
    "    for I in index:\n",
    "        if mode == 'CV':\n",
    "            chosen = random.sample(index[I],minimumPoint)\n",
    "        else:\n",
    "            chosen = random.choices(index[I],k=minimumPoint)\n",
    "        for J in chosen:\n",
    "            balancedX.append(X[J])\n",
    "            balancedY.append(y[J])\n",
    "    return balancedX, balancedY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "    listNews = []\n",
    "    for news in text:\n",
    "        listSentence = []\n",
    "        oldText = news\n",
    "        while True:\n",
    "            newText = oldText.replace(\"  \",\" \")\n",
    "            if oldText == newText:\n",
    "                break\n",
    "            oldText = newText\n",
    "        listSentence.extend(newText)\n",
    "        splitText = newText.split(\" \")\n",
    "        listSentence.append(\" \".join([splitText[I] for I in range(0,len(splitText),2)]))\n",
    "        listSentence.append(\" \".join([splitText[I] for I in range(1,len(splitText),2)]))\n",
    "        listNews.append(\" SNIP \".join(listSentence))\n",
    "    return listNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitGroup(kFold):\n",
    "    #Me too lazy to plug in sklearn\n",
    "    assert len(X) == len(y)\n",
    "    index = [int(I) for I in range(len(X))]\n",
    "    random.shuffle(index)\n",
    "    group = [index[len(index)*I//kFold:len(index)*(I+1)//kFold] for I in range(kFold)]\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Custom class for GradientBoosting\n",
    "class Boosting():\n",
    "    def __init__(self):\n",
    "        self.clf = GB()\n",
    "    def fit(self,X,y):\n",
    "        self.clf.fit(X,y)\n",
    "    def predict(self,X):\n",
    "        m = int(X.shape[0] ** (0.5))\n",
    "        pred = []\n",
    "        for I in range(m):\n",
    "            pred.extend(self.clf.predict(X[I*X.shape[0]//m:(I+1)*X.shape[0]//m].toarray()))\n",
    "        return pred\n",
    "#Custom class for K Nearest Neighbor\n",
    "class Neighbors:\n",
    "    def __init__(self):\n",
    "        self.clf = KNC()\n",
    "    def fit(self,X,y):\n",
    "        self.clf.fit(X,y)\n",
    "    def predict(self,X):\n",
    "        m = int(X.shape[0] ** (0.5))\n",
    "        pred = []\n",
    "        for I in range(m):\n",
    "            pred.extend(self.clf.predict(X[I*X.shape[0]//m:(I+1)*X.shape[0]//m]))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bersihkanTeksBerita(array):\n",
    "    hasil = []\n",
    "    for news in array:\n",
    "        newSentence = \"\"\n",
    "        for J in news:\n",
    "            if J.isalpha():\n",
    "                newSentence += J\n",
    "            else:\n",
    "                newSentence += \" \"\n",
    "        hasil.append(newSentence)\n",
    "    return hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfOption = [Boosting(),LR(n_jobs = -1),NB(),LinearSVC(),Neighbors(),RFC()]\n",
    "mrePred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mrc(pred,Y):\n",
    "    assert len(pred)==len(Y)\n",
    "    pred = array(pred)\n",
    "    Y    = array(Y)\n",
    "    \n",
    "    TP, FP , TN, FN = 0,0,0,0\n",
    "    \n",
    "    for I in range(len(pred)):\n",
    "        if pred[I] == Y[I]:\n",
    "            if pred[I] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if pred[I] == -1:\n",
    "                FP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "    print(TP,FP,TN,FN)\n",
    "    try:\n",
    "        return ((TP*TN)-(FP*FN)) / ((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))**(0.5)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data berhasil difetch\n",
      "17280 811 14377\n",
      "Data berhasil difilter\n",
      "1356\n",
      "170 78 107 78\n",
      "153 95 124 61\n",
      "133 115 127 58\n",
      "0 248 185 0\n",
      "127 121 98 87\n",
      "118 130 126 59\n",
      "179 95 93 66\n",
      "168 106 95 64\n",
      "180 94 86 73\n",
      "0 274 159 0\n",
      "147 127 74 85\n",
      "146 128 117 42\n",
      "134 123 125 52\n",
      "173 84 117 60\n",
      "177 80 96 81\n",
      "257 0 0 177\n",
      "143 114 84 93\n",
      "156 101 101 76\n",
      "139 121 109 64\n",
      "155 105 100 73\n",
      "159 101 100 73\n",
      "1 259 173 0\n",
      "142 118 82 91\n",
      "128 132 126 47\n",
      "139 112 127 56\n",
      "155 96 131 52\n",
      "160 91 118 65\n",
      "0 251 183 0\n",
      "126 125 93 90\n",
      "119 132 133 50\n",
      "Mulai training\n",
      "3992 6030 2636 1719\n",
      "19 68 27 9\n",
      "6735 3287 1483 2872\n",
      "80 7 2 34\n",
      "7219 2803 1229 3126\n",
      "86 1 0 36\n",
      "10022 0 0 4355\n",
      "87 0 0 36\n",
      "5393 4629 1962 2393\n",
      "60 27 7 29\n",
      "5643 4379 1967 2388\n",
      "61 26 13 23\n",
      "Data berhasil difetch\n",
      "31657 1766 14396\n",
      "Data berhasil difilter\n",
      "3057\n",
      "446 172 188 158\n",
      "390 228 206 140\n",
      "400 218 197 149\n",
      "374 244 199 147\n",
      "595 23 19 327\n",
      "320 298 234 112\n",
      "448 166 159 192\n",
      "397 217 203 148\n",
      "392 222 202 149\n",
      "0 614 351 0\n",
      "311 303 160 191\n",
      "324 290 229 122\n",
      "484 176 153 151\n",
      "414 246 182 122\n",
      "426 234 182 122\n",
      "660 0 0 304\n",
      "344 316 142 162\n",
      "350 310 202 102\n",
      "448 156 192 169\n",
      "383 221 232 129\n",
      "346 258 228 133\n",
      "0 604 361 0\n",
      "326 278 167 194\n",
      "285 319 265 96\n",
      "452 177 179 157\n",
      "391 238 202 134\n",
      "395 234 211 125\n",
      "629 0 0 336\n",
      "332 297 159 177\n",
      "342 287 217 119\n",
      "Mulai training\n",
      "5142 1887 1941 5426\n",
      "51 8 10 54\n",
      "4636 2393 2398 4969\n",
      "52 7 10 54\n",
      "4366 2663 2689 4678\n",
      "58 1 6 58\n",
      "4654 2375 2358 5009\n",
      "54 5 6 58\n",
      "5845 1184 1250 6117\n",
      "59 0 0 64\n",
      "4757 2272 2147 5220\n",
      "55 4 5 59\n",
      "Data berhasil difetch\n",
      "46053 2604 12724\n",
      "Data berhasil difilter\n",
      "4891\n",
      "448 446 442 163\n",
      "585 309 353 252\n",
      "564 330 360 245\n",
      "0 894 605 0\n",
      "448 446 297 308\n",
      "442 452 417 188\n",
      "688 209 291 311\n",
      "533 364 366 236\n",
      "552 345 360 242\n",
      "897 0 0 602\n",
      "464 433 270 332\n",
      "522 375 408 194\n",
      "685 230 274 310\n",
      "553 362 362 222\n",
      "543 372 350 234\n",
      "1 914 584 0\n",
      "451 464 283 301\n",
      "455 460 414 170\n",
      "429 488 437 145\n",
      "562 355 352 230\n",
      "589 328 352 230\n",
      "0 917 582 0\n",
      "500 417 265 317\n",
      "491 426 412 170\n",
      "662 214 303 320\n",
      "541 335 396 227\n",
      "527 349 396 227\n",
      "876 0 0 623\n",
      "406 470 301 322\n",
      "490 386 428 195\n",
      "Mulai training\n",
      "1497 3850 5729 1648\n",
      "6 45 65 5\n",
      "2863 2484 3720 3657\n",
      "33 18 45 25\n",
      "2786 2561 3778 3599\n",
      "32 19 38 32\n",
      "5 5342 7375 2\n",
      "0 51 70 0\n",
      "3000 2347 3213 4164\n",
      "43 8 9 61\n",
      "2449 2898 4316 3061\n",
      "20 31 48 22\n",
      "Data berhasil difetch\n",
      "58777 3276 10900\n",
      "Data berhasil difilter\n",
      "6397\n",
      "871 207 334 522\n",
      "666 412 516 340\n",
      "691 387 498 358\n",
      "0 1078 856 0\n",
      "552 526 429 427\n",
      "613 465 553 303\n",
      "581 521 596 237\n",
      "679 423 528 305\n",
      "684 418 495 338\n",
      "0 1102 833 0\n",
      "500 602 426 407\n",
      "569 533 603 230\n",
      "589 521 593 231\n",
      "713 397 506 318\n",
      "744 366 477 347\n",
      "0 1110 824 0\n",
      "558 552 413 411\n",
      "606 504 563 261\n",
      "835 239 397 464\n",
      "695 379 550 311\n",
      "712 362 517 344\n",
      "0 1074 861 0\n",
      "566 508 439 422\n",
      "577 497 595 266\n",
      "910 199 332 494\n",
      "731 378 480 346\n",
      "727 382 453 373\n",
      "1109 0 0 826\n",
      "595 514 385 441\n",
      "645 464 559 267\n",
      "Mulai training\n",
      "3277 1818 2374 3431\n",
      "45 12 25 43\n",
      "2485 2610 3287 2518\n",
      "29 28 46 22\n",
      "2771 2324 2844 2961\n",
      "40 17 32 36\n",
      "69 5026 5720 85\n",
      "0 57 68 0\n",
      "2680 2415 2736 3069\n",
      "38 19 23 45\n",
      "1984 3111 3737 2068\n",
      "17 40 54 14\n",
      "Data berhasil difetch\n",
      "69677 3782 10074\n",
      "Data berhasil difilter\n",
      "7614\n",
      "907 365 570 437\n",
      "788 484 636 371\n",
      "834 438 612 395\n",
      "1272 0 0 1007\n",
      "721 551 444 563\n",
      "729 543 672 335\n",
      "908 338 513 520\n",
      "791 455 648 385\n",
      "844 402 586 447\n",
      "1246 0 0 1033\n",
      "673 573 494 539\n",
      "689 557 713 320\n",
      "892 343 537 507\n",
      "786 449 653 391\n",
      "804 431 635 409\n",
      "0 1235 1044 0\n",
      "688 547 438 606\n",
      "699 536 699 345\n",
      "846 373 602 458\n",
      "776 443 658 402\n",
      "819 400 609 451\n",
      "1 1218 1060 0\n",
      "724 495 459 601\n",
      "690 529 765 295\n",
      "953 323 497 507\n",
      "803 473 609 395\n",
      "856 420 565 439\n",
      "0 1276 1004 0\n",
      "648 628 459 545\n",
      "740 536 678 326\n",
      "Mulai training\n",
      "3715 1984 1495 2880\n",
      "52 10 12 44\n",
      "3278 2421 2000 2375\n",
      "42 20 24 32\n",
      "3753 1946 1699 2676\n",
      "60 2 11 45\n",
      "5671 28 31 4344\n",
      "62 0 0 56\n",
      "3031 2668 2079 2296\n",
      "42 20 21 35\n",
      "2951 2748 2138 2237\n",
      "33 29 28 28\n"
     ]
    }
   ],
   "source": [
    "mrePred = []\n",
    "for iter in range(5):\n",
    "    mreTotal = []\n",
    "    query = \"Select * from berita WHERE Date <= \"+str(date[iter])+\" AND Title LIKE '%ekono%' \"\n",
    "    c.execute(query)\n",
    "    trainData = c.fetchall()\n",
    "    \n",
    "    query = \"Select * from berita WHERE Date <= \"+str(date[iter])+\" AND NOT Title LIKE '%ekono%' \"\n",
    "    c.execute(query)\n",
    "    trainDataUnknown = c.fetchall()\n",
    "    \n",
    "    query = \"Select * from berita WHERE Date <= \"+str(date[iter+1])+\" AND \"+str(date[iter])+\"< Date AND NOT Title LIKE '%ekono%' \"\n",
    "    c.execute(query)\n",
    "    testData = c.fetchall()\n",
    "    \n",
    "    print(\"Data berhasil difetch\")\n",
    "    print(len(trainDataUnknown),len(trainData),len(testData))\n",
    "    filtered = []\n",
    "    for I in range(0,len(trainDataUnknown),len(trainData)):\n",
    "        X = [J[3] for J in trainData]\n",
    "        y = [int(1) for J in trainData]\n",
    "        toEvaluate = [J[3] for J in trainDataUnknown[I:I+len(trainData)]]\n",
    "        X += toEvaluate\n",
    "        y += [int(0) for J in toEvaluate]\n",
    "    \n",
    "        counter = CV()\n",
    "        vector = counter.fit_transform(bersihkanTeksBerita(X))\n",
    "        toEvaluateVector = counter.transform(bersihkanTeksBerita(toEvaluate))\n",
    "    \n",
    "        bayes = NB()\n",
    "        bayes.fit(vector,y)\n",
    "        predict = bayes.predict_proba(toEvaluateVector)\n",
    "    \n",
    "        for J in range(len(predict)):\n",
    "            if predict[J][1] > 0.9:\n",
    "                filtered.append(trainDataUnknown[I+J])\n",
    "    \n",
    "    print(\"Data berhasil difilter\")\n",
    "    \n",
    "    trainData += filtered\n",
    "    print(len(filtered))\n",
    "    # Do cross-validation to choose the best feature selection\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for I in trainData:\n",
    "        X.append(I[3])\n",
    "        y.append(I[7])\n",
    "        \n",
    "    group = splitGroup(5)\n",
    "    XkFold = [[X[J] for J in K]for K in group]\n",
    "    YkFold = [[y[J] for J in K]for K in group]\n",
    "    counterList = []\n",
    "    selectList = []\n",
    "    mreTotal = []\n",
    "  \n",
    "    for I in range(5):\n",
    "        xTrain = []\n",
    "        yTrain = []\n",
    "    \n",
    "        xTest  = []\n",
    "        yTest = []\n",
    "        for J in range(5):\n",
    "            if J == I:\n",
    "                for L in XkFold[J]:\n",
    "                    xTest.append(L)\n",
    "                yTest.extend(YkFold[J])\n",
    "            else:\n",
    "                for L in XkFold[J]:\n",
    "                    xTrain.append(L)\n",
    "                yTrain.extend(YkFold[J])\n",
    "\n",
    "        xTrain = transform(xTrain)\n",
    "        xTest = transform(xTest)\n",
    "\n",
    "        assert len(xTrain) == len(yTrain)\n",
    "        xTrainNew , yTrainNew = balancedTrain(xTrain,yTrain,'CV')\n",
    "        counterList.append(CV(ngram_range=(2,2),min_df=5)) \n",
    "        trainVector = counterList[-1].fit_transform(xTrainNew)\n",
    "        testVector  = counterList[-1].transform(xTest)\n",
    "\n",
    "        selectList.append(SelectKBest(chi2, k = min(10000,trainVector.shape[1])))\n",
    "\n",
    "        trainVector = selectList[-1].fit_transform(trainVector,yTrainNew)\n",
    "        testVector  = selectList[-1].transform(testVector)\n",
    "\n",
    "        mreTotal.append(0)\n",
    "        for J in clfOption:\n",
    "            J.fit(trainVector,yTrainNew)\n",
    "            prediction = J.predict(testVector)\n",
    "            mreTotal[-1] += mrc(prediction,yTest)\n",
    "    \n",
    "    index = mreTotal.index(max(mreTotal))\n",
    "    \n",
    "    mrePred.append({'post':[],\n",
    "                    'chronological':[]})\n",
    "    \n",
    "    xTrainNew = []\n",
    "    yTrainNew = []\n",
    "    # Generating boosting data\n",
    "    for I in range(5):\n",
    "        X_temp , y_temp = balancedTrain(xTrain,yTrain,'Boosting')\n",
    "        xTrainNew.append(X_temp)\n",
    "        yTrainNew.append(y_temp)\n",
    "\n",
    "    trainVector = [counterList[index].transform(I) for I in xTrainNew]        \n",
    "    trainVector = [selectList[index].transform(I)for I in trainVector]\n",
    "        \n",
    "    # Create the test set of chronological entries\n",
    "        \n",
    "    lengthOfTestData = {}\n",
    "    testX = []\n",
    "    dataY = {}\n",
    "\n",
    "    for entry in testData:\n",
    "        if entry[5] not in lengthOfTestData:\n",
    "            lengthOfTestData[entry[5]] = 0\n",
    "            dataY[entry[5]] = entry[7]\n",
    "        lengthOfTestData[entry[5]] += 1\n",
    "        testX.append(entry[3])\n",
    "    \n",
    "    testX = counterList[index].transform(transform(testX))\n",
    "    testVector = selectList[index].transform(testX)\n",
    "    print(\"Mulai training\")\n",
    "    for I in clfOption:\n",
    "        postPredict = array([int(0) for J in range(testVector.shape[0])])\n",
    "        for boostingIter in range(5):\n",
    "            I.fit(trainVector[boostingIter],yTrainNew[boostingIter])\n",
    "            postPredict += I.predict(testVector)\n",
    "        postPredict = [[-1,1][J>0] for J in postPredict]\n",
    "\n",
    "        dayPredict = []\n",
    "        postY = []\n",
    "        dayY = []\n",
    "        sumIndex = 0\n",
    "\n",
    "        for dateTested in lengthOfTestData:               \n",
    "            dayPredict.append([-1,1][sum(postPredict[sumIndex:sumIndex+lengthOfTestData[dateTested]])>0])\n",
    "            postY.extend([dataY[dateTested] for I in range(lengthOfTestData[dateTested])])\n",
    "            dayY.append(dataY[dateTested])\n",
    "            sumIndex += lengthOfTestData[dateTested]\n",
    "\n",
    "        mrePred[-1]['post'].append(mrc(postPredict,postY))\n",
    "        mrePred[-1]['chronological'].append(mrc(dayPredict,dayY))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting -> (chronological) 0.062733 (post) 0.021852 \n",
      "Logistic Regression -> (chronological) 0.118934 (post) 0.024388 \n",
      "Naive Bayes -> (chronological) 0.142702 (post) 0.020623 \n",
      "Linear SVC -> (chronological) 0.003153 (post) 0.000898 \n",
      "K nearest neighbor -> (chronological) -0.019595 (post) -0.001579 \n",
      "Random forest -> (chronological) 0.060045 (post) 0.012550 \n"
     ]
    }
   ],
   "source": [
    "name = [\"Gradient Boosting\",\"Logistic Regression\",\"Naive Bayes\",\"Linear SVC\",\"K nearest neighbor\",\"Random forest\"]\n",
    "for I in range(len(name)):\n",
    "    chronological = [J['chronological'][I] for J in mrePred]\n",
    "    post = [J['post'][I] for J in mrePred]\n",
    "    print(\"%s -> (chronological) %f (post) %f \" % (name[I],sum(chronological)/5,sum(post)/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chronological': [-0.03429916122096134,\n",
       "   -0.04351210017041174,\n",
       "   -0.058238704909845236,\n",
       "   0,\n",
       "   -0.11791289070861165,\n",
       "   0.06087679614953948],\n",
       "  'post': [0.0033853842154468613,\n",
       "   0.012247596968392274,\n",
       "   0.002577444274549272,\n",
       "   0,\n",
       "   -0.01048316037263073,\n",
       "   0.013627675072789504]},\n",
       " {'chronological': [0.02919763719475011,\n",
       "   0.05443709712337166,\n",
       "   0.1656166479917091,\n",
       "   0.015763678553028876,\n",
       "   0,\n",
       "   0.01981412661311067],\n",
       "  'post': [-0.005642207314298505,\n",
       "   -0.015849389766556973,\n",
       "   -0.014328298239338105,\n",
       "   -0.018950864175118622,\n",
       "   0.0016411002274963236,\n",
       "   -0.03446067112631721]},\n",
       " {'chronological': [0.07938841860374447,\n",
       "   0.2865642240726484,\n",
       "   0.1684775569867293,\n",
       "   0,\n",
       "   -0.040201858939733416,\n",
       "   0.080774168684147],\n",
       "  'post': [0.06473423587312536,\n",
       "   0.03921391787731933,\n",
       "   0.03274746053348091,\n",
       "   0.01397705715640349,\n",
       "   -0.0033782855491423717,\n",
       "   0.042910306962324805]},\n",
       " {'chronological': [0.17142857312652204,\n",
       "   0.1877291060740164,\n",
       "   0.1758246421298328,\n",
       "   0,\n",
       "   0.005168898752632221,\n",
       "   0.10652373088084949],\n",
       "  'post': [0.05347062607113149,\n",
       "   0.054036519230877204,\n",
       "   0.033762485679321136,\n",
       "   -0.004649722971039164,\n",
       "   -0.0026746269588858785,\n",
       "   0.034231715250299416]},\n",
       " {'chronological': [0.06794867741305972,\n",
       "   0.10944999909494266,\n",
       "   0.26182955748392633,\n",
       "   0,\n",
       "   0.05497234188277219,\n",
       "   0.03223486228014414],\n",
       "  'post': [-0.006689342600012192,\n",
       "   0.03229343796887487,\n",
       "   0.048355994148943766,\n",
       "   0.014112728232995914,\n",
       "   0.00699816768762661,\n",
       "   0.006442405529775244]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrePred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for I in clfOption:\n",
    "    postPredict = array([int(0) for J in range(testVector.shape[0])])\n",
    "    for boostingIter in range(5):\n",
    "        I.fit(trainVector[boostingIter],yTrainNew[boostingIter])\n",
    "        postPredict += I.predict(testVector)\n",
    "    postPredict = [[-1,1][J>0] for J in postPredict]\n",
    "        \n",
    "    dayPredict = []\n",
    "    postY = []\n",
    "    dayY = []\n",
    "    sumIndex = 0\n",
    "    \n",
    "    for date in lengthOfTestData:               \n",
    "        dayPredict.append([-1,1][sum(postPredict[sumIndex:sumIndex+lengthOfTestData[date]])>0])\n",
    "        postY.extend([dataY[date] for I in range(lengthOfTestData[date])])\n",
    "        dayY.append(dataY[date])\n",
    "        sumIndex += lengthOfTestData[date]\n",
    "            \n",
    "    mrePred[-1]['post'].append(mrc(postPredict,postY))\n",
    "    mrePred[-1]['chronological'].append(mrc(dayPredict,dayY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-23-136562ec89de>, line 2)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-136562ec89de>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    for I in clfOption:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(\"Mulai training\")\n",
    "    for I in clfOption:\n",
    "        postPredict = array([int(0) for J in range(testVector.shape[0])])\n",
    "        for boostingIter in range(5):\n",
    "            I.fit(trainVector[boostingIter],yTrainNew[boostingIter])\n",
    "            postPredict += I.predict(testVector)\n",
    "        postPredict = [[-1,1][J>0] for J in postPredict]\n",
    "        \n",
    "        dayPredict = []\n",
    "        postY = []\n",
    "        sumIndex = 0\n",
    "    \n",
    "        for date in entry:               \n",
    "            dayPredict.append([-1,1][sum(postPredict[sumIndex:sumIndex+len(lengthOfTestData[date])])>0])\n",
    "            postY.extend([label[J] for I in range(len(lengthOfTestData[date]))])\n",
    "            dayY.append(label[J])\n",
    "            sumIndex += len(data[J])\n",
    "            \n",
    "        mrePred[-1]['post'].append(mrc(postPredict,postY))\n",
    "        mrePred[-1]['chronological'].append(mrc(dayPredict,dayY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9013"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainVector.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nJAKARTA, Pengamat ekonomi dari Institute for Development of Economics and Finance (Indef) Enny Sri Hartati, mengungkapkan Perum Bulog paling ideal menguasai 5 juta ton beras per tahun.Dengan pasokan sebesar itu, Enny yakin mafia beras akan hilang dari peredaran. \"Idealnya Bulog 5 juta ton, benar-benar tidak ada mafia beras melakukan spekulasi,\" ujar Enny, Senin (1/6/2015). Enny berpendapat minimal Perum Bulog harus bisa menguasai 10 persen atau 4,5 juta ton dari konsumsi masyarakat sebanyak 45 juta ton. Jika dengan pasokan hingga akhir Mei 2015 hanya 1,2 juta ton, mafia beras bisa cepat melakukan spekulasi harga beras. \"Bulog minimal kuasai 10 persen atau 4,5 juta ton. Itu belum termasuk raskin,\" ungkap Enny. Enny memaparkan kebutuhan untuk raskin (beras untuk masyarakat miskin) juga harus dipenuhi. Angka pasokan ideal untuk raskin menurut Enny sebesar 1,6 juta ton per tahun. \"Kebutuhan raskin harus ada stok untuk cadangan,\" papar Enny. (Adiatmaputra Fajar Pratama)'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
